{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\cbfar\\anaconda3\\envs\\cs221\\lib\\site-packages (1.13.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\cbfar\\anaconda3\\envs\\cs221\\lib\\site-packages (0.14.1)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\cbfar\\anaconda3\\envs\\cs221\\lib\\site-packages (0.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\cbfar\\anaconda3\\envs\\cs221\\lib\\site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: requests in c:\\users\\cbfar\\anaconda3\\envs\\cs221\\lib\\site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\cbfar\\anaconda3\\envs\\cs221\\lib\\site-packages (from torchvision) (9.5.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\cbfar\\anaconda3\\envs\\cs221\\lib\\site-packages (from torchvision) (1.21.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\cbfar\\anaconda3\\envs\\cs221\\lib\\site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\cbfar\\anaconda3\\envs\\cs221\\lib\\site-packages (from requests->torchvision) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\cbfar\\anaconda3\\envs\\cs221\\lib\\site-packages (from requests->torchvision) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\cbfar\\anaconda3\\envs\\cs221\\lib\\site-packages (from requests->torchvision) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erik-luna/miniforge3/envs/cs221_hw7/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/erik-luna/miniforge3/envs/cs221_hw7/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m s_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/s_layer1_v1.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Switch to evaluation mode\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m selector\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms_model_path\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     26\u001b[0m predictor\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(p_model_path))\n\u001b[1;32m     28\u001b[0m resnet\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/miniforge3/envs/cs221_hw7/lib/python3.10/site-packages/torch/serialization.py:999\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    996\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    998\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_like(f, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m--> 999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43m_is_zipfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1000\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n\u001b[1;32m   1004\u001b[0m         overall_storage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/cs221_hw7/lib/python3.10/site-packages/torch/serialization.py:120\u001b[0m, in \u001b[0;36m_is_zipfile\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# Read the first few bytes and match against the ZIP file signature\u001b[39;00m\n\u001b[1;32m    119\u001b[0m local_header_magic_number \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPK\u001b[39m\u001b[38;5;130;01m\\x03\u001b[39;00m\u001b[38;5;130;01m\\x04\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 120\u001b[0m read_bytes \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlocal_header_magic_number\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m f\u001b[38;5;241m.\u001b[39mseek(start)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m read_bytes \u001b[38;5;241m==\u001b[39m local_header_magic_number\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sample import BaseModel, train_predictor, train_selector\n",
    "from sample import PredictorNetwork\n",
    "from sample import SelectorNetwork\n",
    "import torchvision.models as models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# Load the pretrained ResNet-50 model\n",
    "resnet = models.resnet18(pretrained=True)\n",
    "selector = SelectorNetwork(32000)\n",
    "predictor = PredictorNetwork(200704, 32000)\n",
    "resnet.fc = nn.Linear(resnet.fc.in_features, 10)\n",
    "\n",
    "p_model_path = \"models/p_layer1_v1.pth\"\n",
    "s_model_path = \"models/s_layer1_v1.pth\"\n",
    "# Switch to evaluation mode\n",
    "\n",
    "selector.load_state_dict(torch.load(s_model_path))\n",
    "predictor.load_state_dict(torch.load(p_model_path))\n",
    "\n",
    "resnet.eval()\n",
    "selector.eval()\n",
    "predictor.eval()\n",
    "# Transformation and data loading\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "# Define your input data (example input tensor)\n",
    "# Load the validation dataset\n",
    "val_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Iterate through each layer in the ResNet-50 model and apply them sequentially\n",
    "total_start = time.time()\n",
    "\n",
    "for image, labels in val_loader:\n",
    "  output = image\n",
    "  for name, layer in resnet.named_children():\n",
    "      start = time.time()\n",
    "      if name == 'avgpool':\n",
    "          output = nn.functional.adaptive_avg_pool2d(output, (1, 1))\n",
    "      elif name == 'fc':\n",
    "          output = output.view(output.size(0), -1)\n",
    "      elif name == 'layer1':# or name == 'layer2' or name == 'layer3' or name == 'layer4':\n",
    "          output = layer(output)\n",
    "          print(f\"layer1 output shape = {output.shape}\")\n",
    "          pred_out = predictor(output)\n",
    "          if selector(pred_out) == 1:\n",
    "              output = pred_out\n",
    "              break    \n",
    "      else:\n",
    "          output = layer(output)\n",
    "\n",
    "      print(f\"Layer: {name}, Output shape: {output.shape}, total time: {time.time() - start}\")\n",
    "\n",
    "print(f\"total time: {time.time() - total_start}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs221",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
